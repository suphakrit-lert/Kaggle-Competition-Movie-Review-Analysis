{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1806,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "import itertools\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1807,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        return {key: data[key] for key in data}\n",
    "\n",
    "train_data = load_npz(r'.\\data\\train.npz')\n",
    "test_data = load_npz(r'.\\data\\test.npz')\n",
    "train_emb1, train_emb2, train_labels = train_data['emb1'], train_data['emb2'], train_data['preference']\n",
    "test_emb1, test_emb2 = test_data['emb1'], test_data['emb2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uid': array([    0,     1,     2, ..., 18747, 18748, 18749], dtype=int64),\n",
       " 'emb1': array([[-0.05075016, -0.03491386, -0.05787281, ...,  0.00020284,\n",
       "          0.02388327, -0.02491781],\n",
       "        [-0.12402835, -0.07631648, -0.05782915, ...,  0.02713838,\n",
       "          0.01394665,  0.0186507 ],\n",
       "        [-0.06794146, -0.0385992 ,  0.04476113, ...,  0.07999779,\n",
       "          0.04943484,  0.00783883],\n",
       "        ...,\n",
       "        [ 0.02096516, -0.00752076, -0.06958353, ...,  0.01346127,\n",
       "          0.01917063, -0.06059628],\n",
       "        [-0.00901941,  0.01330765, -0.02343761, ..., -0.02690429,\n",
       "          0.0084649 ,  0.01999134],\n",
       "        [-0.05510234,  0.00251053, -0.01775946, ...,  0.00322949,\n",
       "         -0.02700103,  0.01986161]], dtype=float32),\n",
       " 'emb2': array([[-0.03255587,  0.01327268, -0.00508326, ..., -0.01196616,\n",
       "         -0.03564733, -0.03713938],\n",
       "        [-0.00014027,  0.03904634,  0.0592997 , ...,  0.00117963,\n",
       "          0.04012304,  0.07394706],\n",
       "        [-0.068197  , -0.0943828 ,  0.04236921, ...,  0.0225933 ,\n",
       "          0.00185285, -0.03076085],\n",
       "        ...,\n",
       "        [ 0.00845952,  0.00125914, -0.03183057, ..., -0.04645595,\n",
       "         -0.00618974,  0.00794393],\n",
       "        [-0.05969298,  0.00475971,  0.00906092, ..., -0.0083008 ,\n",
       "         -0.05037842, -0.02749569],\n",
       "        [-0.04472147, -0.01137812, -0.05518954, ..., -0.05703627,\n",
       "          0.03633969,  0.00122035]], dtype=float32),\n",
       " 'preference': array([1, 1, 1, ..., 1, 0, 0], dtype=int8)}"
      ]
     },
     "execution_count": 1808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['uid', 'emb1', 'emb2', 'preference'])"
      ]
     },
     "execution_count": 1809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n",
      "(384,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# x1\n",
    "print(train_data['emb1'][0].shape) # (384,)\n",
    "# x2\n",
    "print(train_data['emb2'][0].shape) # (384,)\n",
    "# y\n",
    "print(train_data['preference'][0]) # 1\n",
    "# train_data['emb1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1811,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "\n",
    "# Preprocessing Parameters\n",
    "validation_size = 0.1\n",
    "RAND_STATE = 5780\n",
    "shuffle_split = True\n",
    "standardized = False\n",
    "torch.manual_seed(RAND_STATE)\n",
    "np.random.seed(RAND_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1812,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(Xs, Ys, validation_size: float=0.2):\n",
    "    Xs_tr, Xs_va, Ys_tr, Ys_va = train_test_split(Xs, Ys, test_size=validation_size, random_state=RAND_STATE, shuffle=shuffle_split, stratify=Ys)\n",
    "    return torch.Tensor(Xs_tr), torch.Tensor(Xs_va), torch.Tensor(Ys_tr).long(), torch.Tensor(Ys_va).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1813,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(Xs):\n",
    "    scaler = StandardScaler()\n",
    "    Xs_scaled = scaler.fit_transform(Xs)\n",
    "    return torch.Tensor(Xs_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1814,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18750, 384)\n",
      "(18750, 384)\n",
      "Xs_tr.shape: torch.Size([16875, 768])\n",
      "Ys_tr.shape: torch.Size([16875])\n",
      "Xs_va.shape: torch.Size([1875, 768])\n",
      "Ys_va.shape: torch.Size([1875])\n"
     ]
    }
   ],
   "source": [
    "print(train_data['emb1'].shape) # (n x d): (18750, 384)\n",
    "print(train_data['emb2'].shape) # (n x d): (18750, 384)\n",
    "\n",
    "# Concatenate the input in to a single long vector\n",
    "Xs = np.concatenate((train_data['emb1'], train_data['emb2']), axis=1)\n",
    "Ys = train_data['preference']\n",
    "\n",
    "# Train Validation Split\n",
    "Xs_tr, Xs_va, Ys_tr, Ys_va = train_validation_split(Xs, Ys, validation_size)\n",
    "\n",
    "if standardized:\n",
    "    Xs_tr = standardization(Xs_tr)\n",
    "    Xs_va = standardization(Xs_va)\n",
    "\n",
    "# Convert to Torch\n",
    "print(f'Xs_tr.shape: {Xs_tr.shape}') \n",
    "print(f'Ys_tr.shape: {Ys_tr.shape}')\n",
    "print(f'Xs_va.shape: {Xs_va.shape}')\n",
    "print(f'Ys_va.shape: {Ys_va.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1815,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataset = TensorDataset(Xs_tr, Ys_tr)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataset = TensorDataset(Xs_va, Ys_va)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "torch.use_deterministic_algorithms(True)\n",
    "embedding_dim = 768\n",
    "hidden_dim = 64\n",
    "output_dim = 2\n",
    "num_layers = 5\n",
    "activation = \"relu\"\n",
    "\n",
    "# Improvement\n",
    "include_batch_norm = True\n",
    "initialize_weights = False\n",
    "dropout_rate = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1817,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN Model\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim: int, \n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        activation: str = \"relu\",\n",
    "        num_layers: int = 1,\n",
    "        include_batch_norm: bool = False,\n",
    "        initialize_weights: bool = False,\n",
    "        dropout_rate: Optional[float] = None\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        assert num_layers > 0\n",
    "\n",
    "        # FFNN architecture attributes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Layer attributes\n",
    "        self.input_layer = nn.Linear(self.embedding_dim, self.hidden_dim)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        # Weight initialization attributes\n",
    "        self.initialize_weights = initialize_weights\n",
    "        if initialize_weights:\n",
    "            init.xavier_normal_(self.input_layer.weight)\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                init.xavier_normal_(hidden_layer.weight)\n",
    "            init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "        # FFNN performance improvement attributes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        if dropout_rate is not None:\n",
    "            self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        self.include_batch_norm = include_batch_norm\n",
    "        if include_batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.hidden_dim)\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_layer(embeddings)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            # Forward layer\n",
    "            x = hidden_layer(x)\n",
    "\n",
    "            # Batch normalization layer\n",
    "            if self.include_batch_norm:\n",
    "                x = self.batch_norm(x)\n",
    "\n",
    "            # Non-linear layer\n",
    "            if self.activation == \"relu\":\n",
    "                x = F.relu(x)\n",
    "            elif self.activation == \"tanh\":\n",
    "                x = F.tanh(x)\n",
    "            elif self.activation == \"sigmoid\":\n",
    "                x = F.sigmoid(x)\n",
    "\n",
    "            # Drop out regularization layer\n",
    "            if self.dropout_rate is not None:\n",
    "                x = self.dropout(x)\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (input_layer): Linear(in_features=768, out_features=64, bias=True)\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 1818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "ffnn = FFNN(\n",
    "    embedding_dim=embedding_dim, \n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    activation=activation,\n",
    "    num_layers=num_layers, \n",
    "    include_batch_norm=include_batch_norm,\n",
    "    initialize_weights=initialize_weights,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "ffnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1819,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 50\n",
    "alpha = 0.01\n",
    "rho1 = 0.9\n",
    "rho2 = 0.99\n",
    "grad_clip_max_norm = None\n",
    "\n",
    "# Optimizers\n",
    "# adam_optimizer = torch.optim.Adam(ffnn.parameters(), lr=alpha,  betas = [rho1, rho2])\n",
    "adam_optimizer = torch.optim.Adam(ffnn.parameters(), lr=alpha)\n",
    "\n",
    "# Loss functions\n",
    "cross_entropy_loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1820,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a trained model on MNIST data\n",
    "#\n",
    "# dataloader    dataloader of examples to evaluate on\n",
    "# model         trained PyTorch model\n",
    "# loss_fn       loss function (e.g. torch.nn.CrossEntropyLoss)\n",
    "#\n",
    "# returns       tuple of (loss, accuracy), both python floats\n",
    "@torch.no_grad()\n",
    "def evaluate_model(validation_loader, model, loss_fn):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0.0\n",
    "\ttotal_correct = 0\n",
    "\ttotal_samples = 0\n",
    "\n",
    "\tfor X, Y in validation_loader:\n",
    "\t\tY_pred_prob = model(X)\n",
    "\t\tloss = loss_fn(Y_pred_prob, Y)\n",
    "\t\ttotal_loss += loss.item()\n",
    "\t\n",
    "\t\tY_pred = torch.argmax(Y_pred_prob, dim=1)\n",
    "\t\ttotal_correct += torch.sum(Y_pred == Y).item()\n",
    "\t\ttotal_samples += Y.size(0)\n",
    "\t\n",
    "\taverage_loss = total_loss / len(validation_loader)\n",
    "\taccuracy = total_correct / total_samples\n",
    "\t\n",
    "\treturn average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1821,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "\ttrain_loader, \n",
    "\tvalidation_loader, \n",
    "\tmodel, \n",
    "\tloss_fn, \n",
    "\toptimizer, \n",
    "\tepochs, \n",
    "\tbatch_size, \n",
    "\tgrad_clip_max_norm: Optional[float] = None, \n",
    "\tpatience: Optional[int] = None\n",
    "):\n",
    "\tvalidation_losses = []\n",
    "\tvalidation_accuracies = []\n",
    "\tbest_validation_loss = float(\"inf\")\n",
    "\tno_improvement_count = 0\n",
    "\n",
    "\t# Create DataLoader for batching\n",
    "\tfor epoch in range(epochs):\n",
    "\t\t# Set to training mode\n",
    "\t\tmodel.train()\n",
    "\t\t\n",
    "\t\tfor i, (X, Y) in enumerate(train_loader):\n",
    "\t\t\ttotal_loss = 0.0\n",
    "\n",
    "\t\t\t# Zero gradients for every batch\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# Make predictions for this batch\n",
    "\t\t\tY_pred_prob = model(X)\n",
    "\n",
    "\t\t\t# Compute the loss and its gradients\n",
    "\t\t\tloss = loss_fn(Y_pred_prob, Y)\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\tif grad_clip_max_norm is not None:\n",
    "\t\t\t\tnn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "\n",
    "\t\t\t# Adjust learning weights\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# Gather data and report\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\t\n",
    "\t\t# Evaluate the model\n",
    "\t\tvalidation_loss, validation_accuracy = evaluate_model(validation_loader, model, loss_fn)\n",
    "\t\tvalidation_losses.append(validation_loss)\n",
    "\t\tvalidation_accuracies.append(validation_accuracy)\n",
    "\t\t# print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {round(validation_loss,3)}, Validation Accuracy: {round(validation_accuracy,3)}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "\t\tif validation_loss < best_validation_loss:\n",
    "\t\t\tbest_validation_loss = validation_loss\n",
    "\t\t\tbest_validation_accuracy = validation_accuracy\n",
    "\t\t\tno_improvement_count = 0\n",
    "\t\telse:\n",
    "\t\t\tno_improvement_count += 1\n",
    "\n",
    "\t\tif patience is not None:\n",
    "\t\t\tif no_improvement_count >= patience:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t# best_validation_loss = min(validation_losses)\n",
    "\t# best_validation_accuracy = max(validation_accuracies)\n",
    "\t# print(f\"Minimum Loss: {min_validation_loss}, Max Accuracy: {max_validation_accuracy}\")\n",
    "\treturn model, best_validation_loss, best_validation_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn, best_validation_loss, best_validation_accuracy = train(\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    ffnn, \n",
    "    cross_entropy_loss_fn, \n",
    "    adam_optimizer, \n",
    "    epochs,\n",
    "    batch_size,\n",
    "    grad_clip_max_norm,\n",
    "    patience=5\n",
    ")\n",
    "print(best_validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (input_layer): Linear(in_features=768, out_features=64, bias=True)\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 1682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture Parameters\n",
    "embedding_dim = 768\n",
    "hidden_dim = 128\n",
    "output_dim = 2\n",
    "num_layers = 1\n",
    "activation = \"relu\"\n",
    "\n",
    "# Architecture Improvement Parameters\n",
    "dropout_rate = None\n",
    "include_batch_norm = True\n",
    "initialize_weights = False\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 10\n",
    "alpha = 0.01\n",
    "# rho1 = 0.9\n",
    "# rho2 = 0.999\n",
    "grad_clip_max_norm = None\n",
    "\n",
    "# Optimizers\n",
    "# sgd_optimizer = torch.optim.SGD(ffnn.parameters(), lr=alpha)\n",
    "adam_optimizer = torch.optim.Adam(ffnn.parameters(), lr=alpha)\n",
    "# adamw_optimizer = torch.optim.AdamW(ffnn.parameters(), lr=alpha)\n",
    "# rmsprop_optimizer = torch.optim.RMSprop(ffnn.parameters(), lr=alpha)\n",
    "\n",
    "# Loss functions\n",
    "cross_entropy_loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1661,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_range = 5\n",
    "# FFNN\n",
    "param_grid = {\n",
    "    'hidden_dims': [64], \n",
    "    'activations': [\"relu\"], # relu\n",
    "    'num_layers': [5], \n",
    "    'include_batch_norm': [True], # True\n",
    "    'initialize_weights': [False], # True\n",
    "    'dropout_rates': [None], # 0.5\n",
    "    'batch_sizes': [128, 180], # 180\n",
    "    'grad_clip_max_norms': [None], # 2\n",
    "    'optimizer_types': ['adam'],\n",
    "    'alphas': [0.01],\n",
    "    'epochs': [10],\n",
    "    'rho1': [0.9], # 0.9\n",
    "    'rho2': [0.99] # 0.99\n",
    "}\n",
    "\n",
    "grid_search_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "results = []\n",
    "for (\n",
    "    hidden_dim, \n",
    "    activation, \n",
    "    num_layer, \n",
    "    include_batch_norm, \n",
    "    initialize_weights, \n",
    "    dropout_rate, \n",
    "    # batch_size, \n",
    "    grad_clip_max_norm,\n",
    "    # optimizer_type,\n",
    "    alpha,\n",
    "    epoch,\n",
    "    # rho1,\n",
    "    # rho2,\n",
    "\n",
    "    ) in grid_search_combinations:\n",
    "\n",
    "    sum_best_validation_loss = 0\n",
    "    sum_best_validation_accuracy = 0\n",
    "    for i in range(num_range):\n",
    "        # FFNN Architecture\n",
    "        ffnn = FFNN(\n",
    "            embedding_dim=embedding_dim, \n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim, \n",
    "            num_layers=num_layers, \n",
    "            include_batch_norm=include_batch_norm,\n",
    "            initialize_weights=initialize_weights,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(ffnn.parameters(), lr=alpha)\n",
    "\n",
    "        # Training\n",
    "        best_validation_loss, best_validation_accuracy = train(\n",
    "            train_loader,\n",
    "            validation_loader,\n",
    "            ffnn, \n",
    "            cross_entropy_loss_fn, \n",
    "            optimizer, \n",
    "            epoch,\n",
    "            batch_size,\n",
    "            grad_clip_max_norm,\n",
    "            patience=5\n",
    "        )\n",
    "        sum_best_validation_loss += best_validation_loss\n",
    "        sum_best_validation_accuracy += best_validation_accuracy\n",
    "\n",
    "    # Result\n",
    "    result = dict(\n",
    "        zip(\n",
    "            param_grid.keys(), \n",
    "            (\n",
    "                hidden_dim, \n",
    "                activation, \n",
    "                num_layer,\n",
    "                include_batch_norm, \n",
    "                initialize_weights, \n",
    "                dropout_rate, \n",
    "                grad_clip_max_norm,\n",
    "                alpha,\n",
    "                epoch,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    result[\"avg_best_validation_loss\"] = sum_best_validation_loss / num_range\n",
    "    result[\"avg_best_validation_accuracy\"] = sum_best_validation_accuracy / num_range\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_dims</th>\n",
       "      <th>activations</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>include_batch_norm</th>\n",
       "      <th>initialize_weights</th>\n",
       "      <th>dropout_rates</th>\n",
       "      <th>grad_clip_max_norms</th>\n",
       "      <th>alphas</th>\n",
       "      <th>epochs</th>\n",
       "      <th>avg_best_validation_loss</th>\n",
       "      <th>avg_best_validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.247355</td>\n",
       "      <td>0.893973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden_dims activations  num_layers  include_batch_norm  \\\n",
       "0           64        relu           5                True   \n",
       "\n",
       "   initialize_weights dropout_rates grad_clip_max_norms  alphas  epochs  \\\n",
       "0               False          None                None    0.01      10   \n",
       "\n",
       "   avg_best_validation_loss  avg_best_validation_accuracy  \n",
       "0                  0.247355                      0.893973  "
      ]
     },
     "execution_count": 1662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results)\n",
    "result_df.sort_values(by='avg_best_validation_accuracy', ascending=False).head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_dims</th>\n",
       "      <th>activations</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>best_validation_loss</th>\n",
       "      <th>best_validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>0.289982</td>\n",
       "      <td>0.890667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden_dims activations  num_layers  best_validation_loss  \\\n",
       "3           16        relu           4              0.289982   \n",
       "\n",
       "   best_validation_accuracy  \n",
       "3                  0.890667  "
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.loc[result_df.best_validation_accuracy == result_df.best_validation_accuracy.max(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6096"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.best_validation_accuracy.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"architecture_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN\n",
    "param_grid = {\n",
    "    'hidden_dims': [8, 16], # 8\n",
    "    'activations': [\"relu\"], # relu\n",
    "    'num_layers': [1, 2], # 7\n",
    "    'include_batch_norm': [True], # True\n",
    "    'initialize_weights': [True], # True\n",
    "    'dropout_rates': [0.5, 0.6], # 0.5\n",
    "    'batch_sizes': [128, 180], # 180\n",
    "    'grad_clip_max_norms': [1, 2], # 2\n",
    "    'optimizer_types': ['adam'],\n",
    "    'alphas': [0.001, 0.01],\n",
    "    'epochs': [50, 100],\n",
    "    'rho1': [0.9], # 0.9\n",
    "    'rho2': [0.99] # 0.99\n",
    "}\n",
    "\n",
    "grid_search_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "results = []\n",
    "for (\n",
    "    hidden_dim, \n",
    "    activation, \n",
    "    num_layer, \n",
    "    include_batch_norm, \n",
    "    initialize_weights, \n",
    "    dropout_rate, \n",
    "    batch_size, \n",
    "    grad_clip_max_norm,\n",
    "    optimizer_type,\n",
    "    alpha,\n",
    "    epoch,\n",
    "    rho1,\n",
    "    rho2,\n",
    "\n",
    "    ) in grid_search_combinations:\n",
    "    for i in range(5):\n",
    "        # FFNN Architecture\n",
    "        ffnn = FFNN(\n",
    "            embedding_dim=embedding_dim, \n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim, \n",
    "            num_layers=num_layers, \n",
    "            include_batch_norm=include_batch_norm,\n",
    "            initialize_weights=initialize_weights,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        # Optimizer\n",
    "        # if optimizer_type == 'sgd':\n",
    "        #     optimizer = torch.optim.SGD(ffnn.parameters(), lr=alpha)\n",
    "        # elif optimizer_type == 'rmsprop':\n",
    "        #     optimizer = torch.optim.RMSprop(ffnn.parameters(), lr=alpha)\n",
    "        # elif optimizer_type == 'adam':\n",
    "        #     optimizer = torch.optim.Adam(ffnn.parameters(), lr=alpha, betas = [rho1, rho2])\n",
    "        # elif optimizer_type == 'adamw':\n",
    "        #     optimizer = torch.optim.AdamW(ffnn.parameters(), lr=alpha)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(ffnn.parameters(), lr=alpha)\n",
    "\n",
    "        # Training\n",
    "        best_validation_loss, best_validation_accuracy = train(\n",
    "            train_loader,\n",
    "            validation_loader,\n",
    "            ffnn, \n",
    "            cross_entropy_loss_fn, \n",
    "            optimizer, \n",
    "            epochs,\n",
    "            batch_size,\n",
    "            grad_clip_max_norm,\n",
    "            patience=5\n",
    "        )\n",
    "\n",
    "        # Result\n",
    "        result = dict(\n",
    "            zip(\n",
    "                param_grid.keys(), \n",
    "                (\n",
    "                    hidden_dim, \n",
    "                    activation, \n",
    "                    num_layer, \n",
    "                    include_batch_norm, \n",
    "                    initialize_weights, \n",
    "                    dropout_rate,\n",
    "                    batch_size,\n",
    "                    grad_clip_max_norm,\n",
    "                    optimizer_type,\n",
    "                    alpha,\n",
    "                    epoch,\n",
    "                    rho1,\n",
    "                    rho2\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        result[\"best_validation_loss\"] = best_validation_loss\n",
    "        result[\"best_validation_accuracy\"] = best_validation_accuracy\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1803,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(Xs_te, model):\n",
    "    Y_preds_prob = model(Xs_te)\n",
    "    Y_preds = torch.argmax(Y_preds_prob, axis = 1)\n",
    "    return Y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1804,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(uid, Y_preds):\n",
    "    df = pd.DataFrame({'uid': uid, 'preference': Y_preds})\n",
    "    df.to_csv('results.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1805,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_te = np.concatenate((test_data['emb1'], test_data['emb2']), axis=1)\n",
    "Xs_te = torch.Tensor(Xs_te)\n",
    "Y_preds = make_prediction(Xs_te, ffnn)\n",
    "make_submission(test_data['uid'], np.array(Y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
